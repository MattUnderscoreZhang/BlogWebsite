# Nano Transformer

Transformers used in NLP are way too big and clunky.

I think there are many ways to reduce the computational cost, including by using a convolution-like approach. Instead of comparing each Q and V vector across the entire input at each layer of the transformer, we could instead compare each word with the ones around it in a window, perform a pooling, and repeat on following layers. This would capture the logical structure of language better (first seeing meaning in word phrases, then sentences, then paragraphs, then chapters etc.), and could also reduce the number of traning paramters by about a factor of a hundred.

I still want to try this out, but don't have the time right now. I wouldn't be surprised if this research is already done by someone else soon.

# Notes

try out an ELMo architecture that combines LSTMs so there are fewer per layer
    "https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/#:~:text=ELMo%20is%20a%20novel%20way,as%20well%20as%20the%20industry."
    maybe let the neural net itself indicate the start and end of phrases, to be combined into one at the next layer
    maybe train on wikipedia and split at sentences, then paragraphs, then sections, then whole articles
    maybe replacing the bottom-most layer can do style transfer
    this can be a big improvement in computational power on transformers, which have to dot Q.K across the entire input, rather than surrounding N words
    this can also let us get rid of the position encoding
Wikipedia dataloader
    use python-bdd to write BDD tests

Abandoned on 10/31/2022.
